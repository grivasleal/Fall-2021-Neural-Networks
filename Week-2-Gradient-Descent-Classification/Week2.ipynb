{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "# Lecture 3 - Aug 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares by Gradient Descent\n",
    "\n",
    "We left off last week needing to minimize a loss function for linear regression, i.e. the minimization problem below.\n",
    "\n",
    "$$\\min\\limits_w\\,L(w)=\\min\\limits_w\\,\\|Xw-y\\|^2$$\n",
    "\n",
    "We will use the method of **gradient descent** to find an approximate solution. Gradient descent is a very quick method exploiting some pretty simple ideas from multivariable calculus. While it will only find an approximate answer at best, it is practically good enough in most cases. The benefits are that it is computationally cheap and can be used for many other useful loss functions.\n",
    "\n",
    "Gradient descent (and it's sped-up version, **stochastic gradient descent** or **SGD**) is *heavily* used in machine learning. Along with backpropagation, SGD is the primary method used for training neural networks. In fact, the first practical neural networks were written about in the machine learning literature under a class of methods called \"gradient-based learning\" due to the primacy of SGD and related methods.\n",
    "\n",
    "## Some Ideas from Multivariate Calculus\n",
    "\n",
    "First, we need just a few ideas from multivariate calculus. This is quite minimal, but you learn more details about these topics in sections 4.3 (partial derivatives), 4.6 (gradients), and 4.7 (multivariate optimization) of <a href=\"https://openstax.org/details/books/calculus-volume-3\">*Calculus Volume 3*</a> by Strang.\n",
    "\n",
    "The ideas we need for gradient descent include:\n",
    "\n",
    "* If we have a differentiable function of several variables, like our loss function $L(w) = L(w_0, ..., w_d)$, we can define the **partial derivatives** with respect to each of these variables as\n",
    "\n",
    "    $$L_{w_i}=\\frac{\\partial L}{\\partial w_i}=\\lim\\limits_{h\\to 0}\\frac{L(w+he_i) - L(w)}{h},$$\n",
    "\n",
    "    where $e_i$ is a $(d+1)$-vector with all 0s except for a 1 in the $i$th component.  Geometrically, this partial derivative is the slope of $L$ if we go in the direction of $e_i$.\n",
    "    \n",
    "* To **minimize a multivariable function** by hand, we need to find critical points, which are points $w$ where *all* partial derivatives are 0 and compare which ones give the lowest outputs. In numerical algorithms, must settle for approximations that are \"nearly\" critical points.\n",
    "    \n",
    "* If we put these partial derivatives into a vector of $d+1$ variables, we call that a **gradient**, which we denote\n",
    "    \n",
    "    $$\n",
    "    \\nabla L(w)\n",
    "    =\\begin{pmatrix}\n",
    "    L_{w_0}(w) \\\\\n",
    "    \\vdots \\\\\n",
    "    L_{w_d}(w)\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "* The **directional derivative** of a function in the direction of a unit vector $u$ starting from a point $w$.\n",
    "\n",
    "    $$D_u L(w) = \\lim\\limits_{h\\to 0}\\frac{L(w+hu) - L(w)}{h},$$\n",
    "    \n",
    "    which is the slope in the direction of the vector $u$. Since $L$ is differentiable, this directional derivative will be defined for all directions leaving from $w$. In 1D, there are just two directions: left of right. In 2D, we have directional derivatives at every angle in a circle around the point.\n",
    "    \n",
    "* A common theorem says the **directional derivative is maximized in the direction of the gradient** at each point, so the gradient gives the direction of the *steepest ascent* in the function $L$. Similarly, the direction of the *steepest descent* is $-\\nabla L(w)$, the opposite direction.\n",
    "\n",
    "### The Geometry of Gradient Descent\n",
    "\n",
    "We will discuss the geometry of gradient-based methods in class, but let's discuss a general outline of how gradient descent works, setting aside the stochastic version for now. The goal of gradient descent is to approximately solve the minimization problem\n",
    "\n",
    "$$\\min\\limits_{w}\\,L(w)$$\n",
    "\n",
    "by finding (approximate) critical values by making a guess for the location of a critical value, taking a small step in the opposite direction as the gradient, and repeating this over and over until, hopefully, we reach a minimum value.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "0. Make a guess for the critical value -- $w^0$\n",
    "1. Compute the gradient of $L$ at $w^0$\n",
    "2. Take a small step to $w^1 = w^0 - \\alpha\\nabla L\\left(w^0\\right)$\n",
    "3. Compute the gradient of $L$ at $w^1$\n",
    "4. Take a small step to $w^2 = w^1 - \\alpha\\nabla L\\left(w^1\\right)$\n",
    "5. (repeat until the gradient gets close to $(0, ..., 0)$)\n",
    "\n",
    "This $\\alpha>0$ is a number that will be used in the algorithm as a multiplier of the steps the method will take. This is called the **learning rate**.\n",
    "\n",
    "This idea seems plausible from the calculus ideas above because we just keep switching directions and making a step in the direction of the steepest downward path--the opposite direction as the gradient--until we reach a good place. This is a \"greedy\" algorithm because it just picks the quickest step in each iteration, which is fast, but it is likely to land in the first minimum it finds, which may or may not be optimal.\n",
    "\n",
    "If you had two parameters, $L$ would be like a 3D curved surface. A nice visual to have in mind is a rain drop falling on a huge leaf. The droplet of water will move in the steepest downward direction due to gravity--but this direction *changes* as the drip follows the contours of the leaf. This is what gradient descent does.\n",
    "\n",
    "Will the drip land in the physically lowest altitude part of the leaf? Maybe, but maybe not. If the rain drop lands on the edge, it will probably just roll off the edge. If the leaf has a few different \"sinks,\" different initial locations of the rain drop might cause it to land in these different ones, some of which have lower altitudes than others. Now, if there is heavy rain and lots of rain drops land on the leaf, we can be pretty sure *some* of them will reach the lowest-altitude sink.\n",
    "\n",
    "From this analogy, you might get the idea that we can make several initial guesses and run it to be more confident we will find the global minimum and not just a local minimum.\n",
    "\n",
    "In the end, if some of our initial guesses are good choices, the step size $\\alpha$ is not too big or too small, and the loss function is pretty well-behaved, the method will converge approximately to a local minimum.\n",
    "\n",
    "### Implementing Gradient Descent\n",
    "\n",
    "Before writing code for gradient descent, let's import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
    "np.set_printoptions(linewidth=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement gradient descent and test it.\n",
    "\n",
    "Gradient descent will need a few inputs:\n",
    "\n",
    "* A function to minimize $f$\n",
    "* A starting point $x_0$\n",
    "* A learning rate $\\alpha$\n",
    "* A small number $h$ (the variable that goes to 0 in the definition of the derivative)\n",
    "* A small positive value that we can use for a stopping condition for the derivative being sufficiently small (the tolerance)\n",
    "* A maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the gradient\n",
    "def computeGradient(f, x, h):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    \n",
    "    for counter in range(n):\n",
    "        xUp = x.copy()\n",
    "        xUp[counter] += h\n",
    "        gradient[counter] = (f(xUp) - f(x))/h\n",
    "            \n",
    "    return gradient\n",
    "\n",
    "# run gradient descent ant output the \n",
    "def gradientDescent(f, x0, alpha, h, tolerance, maxIterations):\n",
    "    # set x equal to the initial guess\n",
    "    x = x0\n",
    "                \n",
    "    # take up to maxIterations number of steps\n",
    "    for counter in range(maxIterations):\n",
    "        # update the gradient\n",
    "        gradient = computeGradient(f, x, h)\n",
    "        \n",
    "        # stop if the norm of the gradient is near 0\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            print('Gradient descent took', counter, 'iterations to converge')\n",
    "            print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "            # return the approximate critical value x\n",
    "            return x\n",
    "        \n",
    "        # if we do not converge, print a message\n",
    "        elif counter == maxIterations-1:\n",
    "            print(\"Gradient descent failed\")\n",
    "            print('The gradient is', gradient)\n",
    "            # return x, sometimes it is still pretty good\n",
    "            return x\n",
    "        \n",
    "        # take a step in the opposite direction as the gradient\n",
    "        x -= alpha*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 10 iterations to converge\n",
      "The norm of the gradient is 4.5055999998641627e-07\n",
      "[-0.19999977] 0.03999990988805076\n"
     ]
    }
   ],
   "source": [
    "f = lambda x : x[0]**2\n",
    "\n",
    "x = gradientDescent(f,[2],0.4,0.4,0.000001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 18 iterations to converge\n",
      "The norm of the gradient is 6.221664077488143e-05\n",
      "[4.46232611] [-0.96889687]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.sin(x)\n",
    "\n",
    "x = gradientDescent(f,[2],0.5,0.5,0.0001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a class for linear regression using gradient descent to estimate $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresGradient:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, w0, alpha, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.initialGuess = w0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the w values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)\n",
    "        self.w = self.gradientDescent(L, self.initialGuess, self.alpha, self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of w from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.w @ X[row,]\n",
    "            \n",
    "        return yPredicted\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, h, tolerance, maxIterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == maxIterations-1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "    # estimate the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - f(x))/h\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "\n",
      "Gradient descent took 17180 iterations to converge\n",
      "The norm of the gradient is 0.009999533018135179\n",
      "\n",
      "The predicted y values are [1.88927362 2.39728623 2.90529883 3.41331144 2.39728623]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The w values are [-1.15880201  0.5080126 ]\n",
      "\n",
      "The r^2 score is 0.2890134587732638\n",
      "The mean squared error is 0.7394260028758058\n",
      "The mean absolute error is 0.6794572453892247 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [7]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "# instantiate an least squares object, fit to data, predict data\n",
    "model = LeastSquaresGradient()\n",
    "\n",
    "print('Fitting the model...\\n')\n",
    "model.fit(X, y, [0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('\\nThe predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the w values\n",
    "parameters = model.w\n",
    "print('The w values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Data')\n",
    "\n",
    "# plot the fitted model with the data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "\n",
    "# write a string for the formula\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "\n",
    "# plot the model\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('\\nThe r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model seems to work. It's not a great fit, although the data is not really linear, so the model cannot fit it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation: Train, Dev, and Test Datasets\n",
    "\n",
    "(see the lecture notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: High School Graduate Rates in US States\n",
    "\n",
    "Let's try to use least squares on a real dataset. The CSV file in `../data/US_State_data.csv` contains data from each U.S. state.\n",
    "\n",
    "We would like to predict the output variable included, the high school graduation rate, from some input variables: including the crime rate (per 100,000 persons), the violent crime rate (per 100,000 persons), average teacher salary, student-to-teacher ratio, education expenditure per student, population density, and median household income.\n",
    "\n",
    "This means we have 50 examples (one for each state), 7 input (predictor) variables, and one output (response) variable. In order to use the formula we derived above to attack the problem with least squares, we need to find the matrices $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "\n",
      "Gradient descent took 786 iterations to converge\n",
      "The norm of the gradient is 0.009904894592102644\n",
      "\n",
      "The r^2 score is 0.39216258491472866\n",
      "\n",
      "The mean absolute error on the training set is 3.7965640305341184\n",
      "\n",
      "The predicted y values for the test set are [81. 80. 89. 86. 81. 82. 85. 90. 76. 84. 84. 80. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "\n",
      "The weights are [82.97247297 -3.79844358  0.52284534 -1.27291957 -1.296429   -1.71354363  2.21637078]\n",
      "\n",
      "The mean absolute error on the test set is 4.050315901439818 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pandas.read_csv('../data/US_State_Data.csv', sep=',').to_numpy()\n",
    "\n",
    "X = np.array(data[:,1:7], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "trainX = scale(trainX)\n",
    "testX = scale(testX)\n",
    "\n",
    "# instantiate a least squares model\n",
    "model = LeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the w parameters)\n",
    "print('Fitting the model...\\n')\n",
    "model.fit(trainX, trainY, [0, 0, 0, 0, 0, 0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nThe mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('\\nThe predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the weights\n",
    "print('\\nThe weights are', model.w)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nThe mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Gradient Descent\n",
    "\n",
    "* We must be careful with the $h$ and tolerance hyperparameters to be sure gradient descent will converge.\n",
    "\n",
    "* Gradient descent in our implementation above does not actually require any derivatives since we only used approximate derivatives.\n",
    "\n",
    "* If we knew formulas for the derivatives, we could compute them exactly to let the step size be exactly proportional to $\\nabla L$. This would drastically reduce the number of times we compute the loss function and increase speed.\n",
    "\n",
    "* We will use exact derivatives as well as other approaches to speed up gradient descent when we start to build huge neural networks.\n",
    "\n",
    "* Gradient descent and related methods are the main driver of many machine learning problems that are based on to minimizing a loss function (least squares and neural networks, among others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Regularization** is an approach of adjusting a model in various ways to deal with collinearity among other problems. Another important effect of regularization is that it can sometimes reduce overfitting, a problem where we fit the model to the training data too strongly causing it to perform badly on test data.\n",
    "\n",
    "For example, in the image below, a regularized model would look more like the green curve, which is much simpler and *probably* captures the real dynamics of the system generating the data better than the blue curve, even though they both fit the training data (the red dots) perfectly.\n",
    "\n",
    "![regularization diagram](../images/regularization.png)\n",
    "\n",
    "In general, regularization typically sacrifices some training accuracy through approaches that reduce the dimension of the parameter space, shrink some parameters, and adjust loss functions in such a way that it improves how well the model generalizes to test data and other unknown data.\n",
    "\n",
    "We will want to add regularization to our toolboxfor reducing overfitting whenever we learn by minimizing a loss function, which is what neural networks do. There are a few common regularization methods use in linear regression, which mirror its usage in neural networks, where they are commonly referred to as **weight decay**, which we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "One issue that can cause overfitting is when parameters get far larger than other parameters, which can over-emphasize certain features. **Ridge regression** (also called **Tikhonov regularization** or $L^2$ **regularization**) offers a partial remedy for the problems of overfitting by adjusting the loss function to \"encourage\" the $w_i$'s to be smaller by **penalizing** large $w_i$'s. To do this, instead of the ordinary least squares loss function\n",
    "\n",
    "$$L(w)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2= \\|Xw-y\\|_2^2,$$\n",
    "\n",
    "we add another term to the loss function to get\n",
    "\n",
    "$$L_{\\text{ridge}}(\\lambda,w)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 + \\lambda\\sum\\limits_{i=1}^dw_i^2=\\|Xw-y\\|_2^2+\\lambda\\|w\\|_2^2,$$\n",
    "\n",
    "for some constant $\\lambda\\geq 0$. So, here, if $w_i$ is large, the loss will be larger. Therefore, when we minimize the loss function, it will push $w_i$'s toward 0 unless there is a really good reason not to do so. That's why we say we *penalize* large $w$ values.\n",
    "\n",
    "The value $\\lambda$ is a hyperparameter of ridge regression. As $\\lambda\\to 0$, ridge regression approaches ordinary least squares. If $\\lambda$ is very large, optimizing the loss function will force the $w$ values to be small. So, the larger we make $\\lambda$, the more pressure we put on the $w$ parameters to shrink.\n",
    "\n",
    "### Note\n",
    "\n",
    "For ridge regression, you typically should normalize the data before minimizing the loss function. Otherwise, different scaling will cause minimization to penalize some variables more than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression and Elastic Net Regression\n",
    "\n",
    "**LASSO** (least absolute shrinkage and selection operator) regression is very similar to ridge regression, but it can otherwise be called **$L^1$ regularization** because it adds an $L^1$ penalty to the size of the $w$ parameters, so the loss function is\n",
    "\n",
    "$$L_{\\text{lasso}}(\\lambda,w)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 + \\lambda\\sum\\limits_{i=1}^d |w_i|=\\|Xw-y\\|_2^2+\\lambda\\|w\\|_1.$$\n",
    "\n",
    "**Elastic net** regression combines both $L^1$ and $L^2$ regularization as a linear combination. So, the elastic net loss function is\n",
    "\n",
    "$$L_{\\text{elastic}}(\\lambda_1,\\lambda_2,w)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 + \\lambda_1\\sum\\limits_{i=1}^d |w_i| + \\lambda_2\\sum\\limits_{i=1}^d w_i^2=\\|Xw-y\\|_2^2+\\lambda_1\\|w\\|_1+\\lambda_2\\|w\\|_2^2,$$\n",
    "\n",
    "so it contains two hyperparameters $\\lambda_1\\geq 0$ and $\\lambda_2\\geq 0$ controlling how large the $L^1$ and $L^2$ penalties are.\n",
    "\n",
    "## Ridge vs. LASSO vs. Elastic Net Regression\n",
    "\n",
    "Ridge, LASSO, and elastic net regression aim to accomplish most of the same tasks:\n",
    "\n",
    "1. Predict outputs when there are linearly dependent variables\n",
    "2. Reduce the dimension of the data (practically speaking)\n",
    "3. improve how well a model can generalize to test data and beyond\n",
    "\n",
    "So, which one should we use? That's not really an easy question to answer because the usefulness of a model depends on how well it can generalize to unknown datapoints, but they are unknown, so... we don't know much about them!\n",
    "\n",
    "There are some differences between $L^1$ and $L^2$ penalties that can guide our testing, although only testing is likely to tell us what will work better, practically speaking.\n",
    "\n",
    "* A loss function with an $L^1$ penalty is NOT differentiable when any $w_i=0$, so we have to be careful with gradient-based methods. A method called soft-thresholding is often used to send parameters directly to 0 if the gradient method brings a $w_i$ sufficiently close close to 0.\n",
    "\n",
    "* A loss function with an $L^2$ penalty will not cause parameters to go to 0, but $L^1$ can. (We will talk about some of the geometry of why this is true in class. It's also discussed in some videos about <a href=\"https://www.youtube.com/watch?v=5asL5Eq2x0A\">ridge</a> and <a href=\"https://www.youtube.com/watch?v=jbwSCwoT51M\">LASSO</a> regression.)\n",
    "\n",
    "* There is no simple formula for $w$ minimizing the loss functions in LASSO or elastic net regression, so numerical optimization, such as gradient descent, must be used.\n",
    "\n",
    "* There is a matrix expression for $w$ minimizing the loss function in ridge regression, but gradient descent can be used as well.\n",
    "\n",
    "This means $L^1$ can totally eliminate variables, which can be good or bad depending on what we are modeling. If the output is well-predicted by only a few variables, this is good. If we need lots of variables to predict the output, this is bad.\n",
    "\n",
    "These notes only provide a short intro to regularization techniques, but you can read more from the (free) classic book <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">*Elements of Statistical Learning*</a> by Hastie, et. al., in section 3.4.\n",
    "\n",
    "## Implementing Regularized Linear Regression\n",
    "\n",
    "To implement these methods, we simply adjust the loss function, and gradient descent will still work. Previously, we had\n",
    "\n",
    "`L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)`\n",
    "\n",
    "For ridge regression, simply add a hyperparameter $\\lambda_2$ times the sum of squares of the weights:\n",
    "\n",
    "`L = lambda w: ((W @ w).T - y.T) @ (X @ x - y) + l2 * w.T @ w`\n",
    "\n",
    "For LASSO regression, simply add a hyperparameter $\\lambda_1$ times the sum of absolute values of the weights:\n",
    "\n",
    "`L = lambda w: ((W @ w).T - y.T) @ (X @ x - y) + l1 * np.sum(np.abs(w))`\n",
    "\n",
    "For elastic net, add both parts:\n",
    "\n",
    "`L = lambda w: ((W @ w).T - y.T) @ (X @ x - y) + l1 * np.sum(np.abs(w)) + l2 * w.T @ w`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
