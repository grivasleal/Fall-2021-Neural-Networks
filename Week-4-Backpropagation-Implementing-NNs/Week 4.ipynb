{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "# Lecture 7 - Sept 16 - Stochastic Gradient Descent\n",
    "\n",
    "We already know what gradient descent is: we run our model on training data, compute the loss function, find the gradient of the loss function, and update our parameters in the opposite direction of the gradient by a small amount; and repeat this until the model converges. The same general approach is used in nearly all neural networks.\n",
    "\n",
    "In linear regression and logistic classification, we were computing the outputs and loss function for the entire dataset (usually multiple times) per iteration of gradient descent, making a weight update based on the gradient, and repeating.\n",
    "\n",
    "The typical approach in neural networks is **stochastic** gradient descent (SGD). SGD updates weights after processing a random sample, called a **mini-batch**, of datapoints: enough points to reduce the variance of using just one for more stable convergence of parameters but not so much that the computation is too expensive. In this way, we process a mini-batch for outputs, compute an approximate loss function and approximate gradients, update weights, and repeat.\n",
    "\n",
    "#### How large should our mini-batches be?\n",
    "\n",
    "Typically, using $2^n$ for something like $n=5, 6, 7, 8$ because these values tend to be ideal for the linear algebra optimization libraries we use (NumPy, TensorFlow, etc). The mini-batch size is a hyperparameter, but it generally isn't one you need to tweak too much.\n",
    "\n",
    "One exception is if you are using GPUs for computing: then, it's best to choose the largest power of 2 that allows a whole mini-batch to fit into GPU memory.\n",
    "\n",
    "## Implementing SGD\n",
    "\n",
    "Once we implement SGD with backpropagation, we will have constructed a \"vanilla\" neural network, which is probably the simplest neural network that is of practical use.\n",
    "\n",
    "First, let's import some libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some more libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write a class similar to the `FeedforwardNeuralNetwork` class we wrote previously but upgraded to use SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    \n",
    "    # input a vector [a, b, c, ...] with the number of nodes in each layer\n",
    "    def __init__(self, layers, alpha = 0.1, batchSize = 32):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # initialize the weights (randomly) -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            self.W.append(np.random.randn(layers[i] + 1, layers[i + 1] + 1))\n",
    "            \n",
    "        # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "        self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
    "        \n",
    "    # define the sigmoid activation\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    # define the sigmoid derivative (where z is the output of a sigmoid)\n",
    "    def sigmoidDerivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, epochs = 10000, update = 1000):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "\n",
    "        for epoch in np.arange(0,epochs):\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0,X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.sigmoid(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    \n",
    "                # backpropagation (coming soon!)\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                D = [error * self.sigmoidDerivative(A[-1])]\n",
    "                \n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.sigmoidDerivative(A[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
    "                    \n",
    "            if (epoch + 1) % update == 0:\n",
    "                loss = self.computeLoss(X,y)\n",
    "                print('Epoch =', epoch + 1, '\\t loss =', loss)\n",
    "                \n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        loss = np.sum((predictions - y)**2) / 2.0\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST\n",
    "\n",
    "As promised, this SGD neural net should run faster, so let's try to use the full 60,000 training images available in MNIST and 10,000 test images. (This is still a LOT of computation, using 70000 total 28-by-28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 loss = 5114.252384701645\n",
      "Epoch = 2 loss = 3827.640450070584\n",
      "Epoch = 3 loss = 3473.3288513337393\n",
      "Epoch = 4 loss = 2993.842063360823\n",
      "Epoch = 5 loss = 2901.9404384637314\n",
      "Epoch = 6 loss = 2630.701667897917\n",
      "Epoch = 7 loss = 2464.5746902285573\n",
      "Epoch = 8 loss = 2795.425929393308\n",
      "Epoch = 9 loss = 2364.3985255987027\n",
      "Epoch = 10 loss = 2353.410652586407\n",
      "Epoch = 11 loss = 2215.9648653618037\n",
      "Epoch = 12 loss = 2128.7799963404223\n",
      "Epoch = 13 loss = 2262.336653846681\n",
      "Epoch = 14 loss = 1974.6134364267582\n",
      "Epoch = 15 loss = 1940.664549489394\n",
      "Epoch = 16 loss = 2430.3313501223215\n",
      "Epoch = 17 loss = 2075.9776717066957\n",
      "Epoch = 18 loss = 2329.289774857599\n",
      "Epoch = 19 loss = 1993.931256521833\n",
      "Epoch = 20 loss = 1799.237987718006\n",
      "Epoch = 21 loss = 1747.4760865779667\n",
      "Epoch = 22 loss = 1755.3602538310915\n",
      "Epoch = 23 loss = 1909.9185042697188\n",
      "Epoch = 24 loss = 1860.02112009211\n",
      "Epoch = 25 loss = 1821.339117683481\n",
      "Epoch = 26 loss = 1844.2668468960146\n",
      "Epoch = 27 loss = 1662.565576476386\n",
      "Epoch = 28 loss = 1828.9521264206987\n",
      "Epoch = 29 loss = 1926.414791181462\n",
      "Epoch = 30 loss = 1622.635249632976\n",
      "Epoch = 31 loss = 1687.1826722341666\n",
      "Epoch = 32 loss = 1510.8419745902745\n",
      "Epoch = 33 loss = 1484.0499705336379\n",
      "Epoch = 34 loss = 1660.9501294972274\n",
      "Epoch = 35 loss = 1503.9626019008072\n",
      "Epoch = 36 loss = 1490.3109687805436\n",
      "Epoch = 37 loss = 1599.7191946003888\n",
      "Epoch = 38 loss = 1461.3507954358042\n",
      "Epoch = 39 loss = 1621.275642912683\n",
      "Epoch = 40 loss = 1444.7766004882224\n",
      "Epoch = 41 loss = 1410.2368800679865\n",
      "Epoch = 42 loss = 1616.9435023963501\n",
      "Epoch = 43 loss = 1348.2703087436303\n",
      "Epoch = 44 loss = 1848.6570281302577\n",
      "Epoch = 45 loss = 1577.1092088193052\n",
      "Epoch = 46 loss = 1510.870190363473\n",
      "Epoch = 47 loss = 1406.4326198755389\n",
      "Epoch = 48 loss = 1460.894174320443\n",
      "Epoch = 49 loss = 1566.7977650538458\n",
      "Epoch = 50 loss = 1365.9266919553145\n",
      "Epoch = 51 loss = 1336.0557729110642\n",
      "Epoch = 52 loss = 1389.8273061964442\n",
      "Epoch = 53 loss = 1514.7330184486227\n",
      "Epoch = 54 loss = 1401.3044100375325\n",
      "Epoch = 55 loss = 1316.4457293590162\n",
      "Epoch = 56 loss = 1278.9040967323847\n",
      "Epoch = 57 loss = 1336.3043801752149\n",
      "Epoch = 58 loss = 1370.8269588383225\n",
      "Epoch = 59 loss = 1158.834571840838\n",
      "Epoch = 60 loss = 1401.4670505932504\n",
      "Epoch = 61 loss = 1418.414063653836\n",
      "Epoch = 62 loss = 1384.8042826630692\n",
      "Epoch = 63 loss = 1276.41016098969\n",
      "Epoch = 64 loss = 1448.4473275062608\n",
      "Epoch = 65 loss = 1312.9833002366734\n",
      "Epoch = 66 loss = 1262.940780245831\n",
      "Epoch = 67 loss = 1257.0394151322962\n",
      "Epoch = 68 loss = 1706.8856972615536\n",
      "Epoch = 69 loss = 1317.984312729474\n",
      "Epoch = 70 loss = 1273.9877852975926\n",
      "Epoch = 71 loss = 1093.9643866091915\n",
      "Epoch = 72 loss = 1321.4498640633585\n",
      "Epoch = 73 loss = 1137.8832326738352\n",
      "Epoch = 74 loss = 1299.2896668228004\n",
      "Epoch = 75 loss = 1182.9337016981935\n",
      "Epoch = 76 loss = 1233.7441907511172\n",
      "Epoch = 77 loss = 1226.3102357965165\n",
      "Epoch = 78 loss = 1137.2027107557876\n",
      "Epoch = 79 loss = 1461.163035237853\n",
      "Epoch = 80 loss = 1335.699370097802\n",
      "Epoch = 81 loss = 1524.9822370427632\n",
      "Epoch = 82 loss = 1121.7278778805778\n",
      "Epoch = 83 loss = 1265.9147558827858\n",
      "Epoch = 84 loss = 1372.7577320546538\n",
      "Epoch = 85 loss = 1255.9770359669906\n",
      "Epoch = 86 loss = 1214.0210069863815\n",
      "Epoch = 87 loss = 1202.3187923097348\n",
      "Epoch = 88 loss = 1138.6461682253075\n",
      "Epoch = 89 loss = 1224.2085727692222\n",
      "Epoch = 90 loss = 1225.5746387693137\n",
      "Epoch = 91 loss = 1245.9589825938483\n",
      "Epoch = 92 loss = 1083.3074920111872\n",
      "Epoch = 93 loss = 1166.547605431623\n",
      "Epoch = 94 loss = 1468.667005014194\n",
      "Epoch = 95 loss = 1214.0119881803223\n",
      "Epoch = 96 loss = 1131.7905609514546\n",
      "Epoch = 97 loss = 1155.8022552430532\n",
      "Epoch = 98 loss = 1121.3988365246678\n",
      "Epoch = 99 loss = 1207.19726478228\n",
      "Epoch = 100 loss = 1288.2108884631305\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      5923\n",
      "           1       0.99      0.98      0.99      6742\n",
      "           2       0.96      0.98      0.97      5958\n",
      "           3       0.96      0.96      0.96      6131\n",
      "           4       0.98      0.99      0.98      5842\n",
      "           5       0.96      0.97      0.96      5421\n",
      "           6       0.98      0.98      0.98      5918\n",
      "           7       0.98      0.98      0.98      6265\n",
      "           8       0.97      0.95      0.96      5851\n",
      "           9       0.97      0.97      0.97      5949\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.95      0.94      0.95      1032\n",
      "           3       0.93      0.95      0.94      1010\n",
      "           4       0.96      0.96      0.96       982\n",
      "           5       0.94      0.94      0.94       892\n",
      "           6       0.95      0.96      0.96       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.95      0.92      0.94       974\n",
      "           9       0.94      0.94      0.94      1009\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# load the full MNIST dataset: both data and labels\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# scale the data to values in [0,1]\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "# reshape the data\n",
    "trainX = trainX.reshape([60000, 28*28])\n",
    "testX = testX.reshape([10000, 28*28])\n",
    "\n",
    "# convert the digits to one-hot vectors\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX, trainY, 100, 1)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test accuracy on MNIST jumped from mid-80\\% previously to 95\\% with our implementation using SGD and the full dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
