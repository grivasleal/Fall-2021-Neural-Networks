{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "\n",
    "This week, we consider Inception nets (Szegedy, et al., 2014 https://arxiv.org/abs/1409.4842), which are CNNs with so-called inception modules, which are small groupings of parallel convolutional layers that act on the same input feature map, processes it through different means, and concatenates the results. In particular, it uses convolutional layers with different-sized filters (1x1, 3x3, and 5x5 in Inception-v1), which are adept at finding patterns at different scales.\n",
    "\n",
    "In this notebook, we implement a small version of Inception and test it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniGoogLeNet\n",
    "\n",
    "Note for GoogLeNet, the inception modules are not exactly sequential, so we will need to define layers a little differently as we see below, after importing some packages. And, we use \"miniception\" modules as per *Understanding Deep Learning Requires Re-Thinking Generalization* (Zhang, et al., 2017 http://arxiv.org/abs/1611.03530), which work well for small-dimensional image datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGoogLeNet:\n",
    "    def convolution_module(x, K, kX, kY, stride, channelsDim, padding=\"same\"):\n",
    "        # create a CONV -> BN -> RELU sequence\n",
    "        x = Conv2D(K, (kX, kY), strides = stride, padding = padding)(x)\n",
    "        x = BatchNormalization(axis = channelsDim)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # return the output\n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numberOf1x1Kernels, numberOf3x3Kernels, channelsDim):\n",
    "        # define two \"parallel\" convolutions of size 1x1 and 3x3 concatenated across the channels dimension\n",
    "        convolution_1x1 = MiniGoogLeNet.convolution_module(x, numberOf1x1Kernels, 1, 1, (1, 1), channelsDim)\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, numberOf3x3Kernels, 3, 3, (1, 1), channelsDim)\n",
    "        x = concatenate([convolution_1x1, convolution_3x3], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def downsample_module(x, K, channelsDim):\n",
    "        # define a CONV and POOL and then concatenate across the channels dimension\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, K, 3, 3, (2, 2), channelsDim, padding = 'valid')\n",
    "        pool = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "        x = concatenate([convolution_3x3, pool], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "        \n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = MiniGoogLeNet.convolution_module(inputs, 96, 3, 3, (1, 1), channelsDim)\n",
    "        \n",
    "        # two inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 32, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 80, channelsDim)\n",
    "        \n",
    "        # four inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 112, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 96, 64, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 80, 80, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 48, 96, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 96, channelsDim)\n",
    "        \n",
    "        # two inception modules followed by global POOL and dropout\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = AveragePooling2D((7, 7))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create a model\n",
    "        model = Model(inputs, x, name='MiniGoogLeNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGoogLeNet on CIFAR-10\n",
    "\n",
    "Let's test it on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Model: \"MiniGoogLeNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 96)   2688        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 96)   384         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 96)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 32)   3104        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   27680       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 64)   0           activation_21[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 32)   2080        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 48)   27696       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 32)   128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 48)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 32)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 48)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 80)   0           activation_23[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 15, 15, 80)   57680       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 15, 15, 80)   320         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 15, 15, 80)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 80)   0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 15, 15, 160)  0           activation_25[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 15, 15, 112)  18032       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 15, 15, 48)   69168       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 15, 15, 112)  448         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 15, 15, 48)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 15, 15, 112)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 15, 15, 48)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 15, 15, 160)  0           activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 15, 15, 96)   15456       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 15, 15, 64)   92224       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 15, 15, 96)   384         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 15, 15, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 15, 15, 96)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 15, 15, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 15, 15, 160)  0           activation_28[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 15, 15, 80)   12880       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 15, 15, 80)   115280      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 15, 15, 80)   320         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 15, 15, 80)   320         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 15, 15, 80)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 15, 15, 80)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 15, 15, 160)  0           activation_30[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 15, 15, 48)   7728        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 15, 15, 96)   138336      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 15, 15, 48)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 15, 15, 96)   384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 15, 15, 48)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 15, 15, 96)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 15, 15, 144)  0           activation_32[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 7, 7, 96)     124512      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 7, 7, 96)     384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 96)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 144)    0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 7, 7, 240)    0           activation_34[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 176)    42416       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 160)    345760      concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 176)    704         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 7, 7, 160)    640         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 176)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 160)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 7, 7, 336)    0           activation_35[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 176)    59312       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 7, 7, 160)    484000      concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 176)    704         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 160)    640         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 176)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 160)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 7, 7, 336)    0           activation_37[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 336)    0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 336)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 336)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           3370        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 10)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,656,250\n",
      "Trainable params: 1,652,826\n",
      "Non-trainable params: 3,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "[INFO] training network...\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/70\n",
      "50000/50000 [==============================] - 34s 672us/sample - loss: 1.3970 - accuracy: 0.4922 - val_loss: 1.2064 - val_accuracy: 0.5635\n",
      "Epoch 2/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.9667 - accuracy: 0.6565 - val_loss: 0.8916 - val_accuracy: 0.6835\n",
      "Epoch 3/70\n",
      "50000/50000 [==============================] - 30s 602us/sample - loss: 0.7824 - accuracy: 0.7249 - val_loss: 1.4288 - val_accuracy: 0.5687\n",
      "Epoch 4/70\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.6626 - accuracy: 0.7679 - val_loss: 0.8465 - val_accuracy: 0.7075\n",
      "Epoch 5/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.5638 - accuracy: 0.8052 - val_loss: 1.1081 - val_accuracy: 0.6550\n",
      "Epoch 6/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.4947 - accuracy: 0.8289 - val_loss: 0.7698 - val_accuracy: 0.7468\n",
      "Epoch 7/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.4252 - accuracy: 0.8549 - val_loss: 0.9700 - val_accuracy: 0.7136\n",
      "Epoch 8/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.3658 - accuracy: 0.8745 - val_loss: 0.6942 - val_accuracy: 0.7770\n",
      "Epoch 9/70\n",
      "50000/50000 [==============================] - 30s 607us/sample - loss: 0.3135 - accuracy: 0.8924 - val_loss: 0.6840 - val_accuracy: 0.7707\n",
      "Epoch 10/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.2574 - accuracy: 0.9119 - val_loss: 0.7111 - val_accuracy: 0.7724\n",
      "Epoch 11/70\n",
      "50000/50000 [==============================] - 30s 610us/sample - loss: 0.2194 - accuracy: 0.9251 - val_loss: 0.7025 - val_accuracy: 0.7824\n",
      "Epoch 12/70\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.1756 - accuracy: 0.9404 - val_loss: 0.7503 - val_accuracy: 0.7855\n",
      "Epoch 13/70\n",
      "50000/50000 [==============================] - 30s 599us/sample - loss: 0.1404 - accuracy: 0.9534 - val_loss: 0.9323 - val_accuracy: 0.7513\n",
      "Epoch 14/70\n",
      "50000/50000 [==============================] - 30s 598us/sample - loss: 0.1238 - accuracy: 0.9587 - val_loss: 0.7178 - val_accuracy: 0.7901\n",
      "Epoch 15/70\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.0974 - accuracy: 0.9671 - val_loss: 0.9593 - val_accuracy: 0.7689\n",
      "Epoch 16/70\n",
      "50000/50000 [==============================] - 30s 608us/sample - loss: 0.0772 - accuracy: 0.9758 - val_loss: 0.7244 - val_accuracy: 0.8005\n",
      "Epoch 17/70\n",
      "50000/50000 [==============================] - 31s 624us/sample - loss: 0.0602 - accuracy: 0.9815 - val_loss: 0.8939 - val_accuracy: 0.7927\n",
      "Epoch 18/70\n",
      "50000/50000 [==============================] - 31s 612us/sample - loss: 0.0561 - accuracy: 0.9828 - val_loss: 1.1533 - val_accuracy: 0.7522\n",
      "Epoch 19/70\n",
      "50000/50000 [==============================] - 31s 613us/sample - loss: 0.0458 - accuracy: 0.9863 - val_loss: 0.9725 - val_accuracy: 0.7854\n",
      "Epoch 20/70\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.0408 - accuracy: 0.9883 - val_loss: 1.1647 - val_accuracy: 0.7719\n",
      "Epoch 21/70\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.7784 - val_accuracy: 0.8074\n",
      "Epoch 22/70\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 0.0225 - accuracy: 0.9938 - val_loss: 0.7252 - val_accuracy: 0.8274\n",
      "Epoch 23/70\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 0.0193 - accuracy: 0.9950 - val_loss: 0.7437 - val_accuracy: 0.8250\n",
      "Epoch 24/70\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.0138 - accuracy: 0.9971 - val_loss: 0.7505 - val_accuracy: 0.8201\n",
      "Epoch 25/70\n",
      "50000/50000 [==============================] - 31s 622us/sample - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.7531 - val_accuracy: 0.8246\n",
      "Epoch 26/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.7191 - val_accuracy: 0.8391\n",
      "Epoch 27/70\n",
      "50000/50000 [==============================] - 30s 602us/sample - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.7115 - val_accuracy: 0.8347\n",
      "Epoch 28/70\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.8043 - val_accuracy: 0.8212\n",
      "Epoch 29/70\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.9630 - val_accuracy: 0.8029\n",
      "Epoch 30/70\n",
      "50000/50000 [==============================] - 30s 601us/sample - loss: 0.0089 - accuracy: 0.9981 - val_loss: 0.7191 - val_accuracy: 0.8356\n",
      "Epoch 31/70\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.7228 - val_accuracy: 0.8344\n",
      "Epoch 32/70\n",
      "50000/50000 [==============================] - 30s 601us/sample - loss: 0.0057 - accuracy: 0.9991 - val_loss: 0.8471 - val_accuracy: 0.8253\n",
      "Epoch 33/70\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 0.0049 - accuracy: 0.9993 - val_loss: 0.7153 - val_accuracy: 0.8414\n",
      "Epoch 34/70\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.7114 - val_accuracy: 0.8406\n",
      "Epoch 35/70\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.7222 - val_accuracy: 0.8401\n",
      "Epoch 36/70\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.7341 - val_accuracy: 0.8367\n",
      "Epoch 37/70\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.7205 - val_accuracy: 0.8427\n",
      "Epoch 38/70\n",
      "50000/50000 [==============================] - 31s 618us/sample - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.7133 - val_accuracy: 0.8439\n",
      "Epoch 39/70\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.7024 - val_accuracy: 0.8452\n",
      "Epoch 40/70\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.7129 - val_accuracy: 0.8453\n",
      "Epoch 41/70\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.7017 - val_accuracy: 0.8429\n",
      "Epoch 42/70\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.7572 - val_accuracy: 0.8355\n",
      "Epoch 43/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.7308 - val_accuracy: 0.8427\n",
      "Epoch 44/70\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.7093 - val_accuracy: 0.8438\n",
      "Epoch 45/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.7037 - val_accuracy: 0.8449\n",
      "Epoch 46/70\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.7035 - val_accuracy: 0.8438\n",
      "Epoch 47/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7043 - val_accuracy: 0.8480\n",
      "Epoch 48/70\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.7032 - val_accuracy: 0.8457\n",
      "Epoch 49/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.7166 - val_accuracy: 0.8476\n",
      "Epoch 50/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7024 - val_accuracy: 0.8476\n",
      "Epoch 51/70\n",
      "50000/50000 [==============================] - 30s 608us/sample - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.7053 - val_accuracy: 0.8456\n",
      "Epoch 52/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7102 - val_accuracy: 0.8469\n",
      "Epoch 53/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.7036 - val_accuracy: 0.8489\n",
      "Epoch 54/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.7026 - val_accuracy: 0.8470\n",
      "Epoch 55/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 0.8485\n",
      "Epoch 56/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.7048 - val_accuracy: 0.8475\n",
      "Epoch 57/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.7039 - val_accuracy: 0.8469\n",
      "Epoch 58/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7034 - val_accuracy: 0.8481\n",
      "Epoch 59/70\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 9.4520e-04 - accuracy: 1.0000 - val_loss: 0.7033 - val_accuracy: 0.8470\n",
      "Epoch 60/70\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.7057 - val_accuracy: 0.8482\n",
      "Epoch 61/70\n",
      "50000/50000 [==============================] - 31s 610us/sample - loss: 9.4762e-04 - accuracy: 1.0000 - val_loss: 0.7033 - val_accuracy: 0.8492\n",
      "Epoch 62/70\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 9.8366e-04 - accuracy: 0.9999 - val_loss: 0.7021 - val_accuracy: 0.8485\n",
      "Epoch 63/70\n",
      "50000/50000 [==============================] - 31s 622us/sample - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.7011 - val_accuracy: 0.8490\n",
      "Epoch 64/70\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 9.6195e-04 - accuracy: 0.9999 - val_loss: 0.7032 - val_accuracy: 0.8479\n",
      "Epoch 65/70\n",
      "50000/50000 [==============================] - 31s 621us/sample - loss: 8.5100e-04 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.8482\n",
      "Epoch 66/70\n",
      "50000/50000 [==============================] - 164s 3ms/sample - loss: 9.8770e-04 - accuracy: 0.9999 - val_loss: 0.7044 - val_accuracy: 0.8487\n",
      "Epoch 67/70\n",
      "50000/50000 [==============================] - 594s 12ms/sample - loss: 8.8851e-04 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.8476\n",
      "Epoch 68/70\n",
      "50000/50000 [==============================] - 590s 12ms/sample - loss: 7.9708e-04 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.8473\n",
      "Epoch 69/70\n",
      "50000/50000 [==============================] - 590s 12ms/sample - loss: 9.4623e-04 - accuracy: 0.9999 - val_loss: 0.7031 - val_accuracy: 0.8481\n",
      "Epoch 70/70\n",
      "50000/50000 [==============================] - 589s 12ms/sample - loss: 9.3028e-04 - accuracy: 0.9999 - val_loss: 0.7031 - val_accuracy: 0.8476\n",
      "\n",
      " Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8530    0.8760    0.8643      1000\n",
      "           1     0.9294    0.9210    0.9252      1000\n",
      "           2     0.7956    0.7630    0.7790      1000\n",
      "           3     0.7381    0.7130    0.7253      1000\n",
      "           4     0.8097    0.8340    0.8217      1000\n",
      "           5     0.7885    0.7680    0.7781      1000\n",
      "           6     0.8674    0.8960    0.8815      1000\n",
      "           7     0.8657    0.8640    0.8649      1000\n",
      "           8     0.9207    0.9290    0.9248      1000\n",
      "           9     0.9003    0.9120    0.9061      1000\n",
      "\n",
      "    accuracy                         0.8476     10000\n",
      "   macro avg     0.8468    0.8476    0.8471     10000\n",
      "weighted avg     0.8468    0.8476    0.8471     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19fea3d2248>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfEpochs = 70\n",
    "initialLearningRate = 0.005\n",
    "\n",
    "def polynomial_decay(epoch):\n",
    "    maxEpochs = numberOfEpochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "    power = 1.0\n",
    "    \n",
    "    alpha = baseLearningRate * (1 - (epoch / float(numberOfEpochs))) ** power\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "    \n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype('float')\n",
    "testX = testX.astype('float')\n",
    "\n",
    "# use mean subtraction\n",
    "mean = np.mean(trainX, axis = 0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "\n",
    "# convert labels to one-hot\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "callbacks = [LearningRateScheduler(polynomial_decay)]\n",
    "\n",
    "print('[INFO] compiling model...')\n",
    "opt = SGD(lr = initialLearningRate, momentum=0.9)\n",
    "model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX, trainY, validation_data = (testX, testY), batch_size = 64, epochs = numberOfEpochs,\n",
    "              callbacks = callbacks, verbose = 1)\n",
    "\n",
    "# save the network to disk\n",
    "#print(\"[INFO] serializing network...\")\n",
    "#model.save('output/MiniGoogLeNet_cifar10.hdf5')\n",
    "\n",
    "# print a classification report\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY, digits=4))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in 85% accuracy, which is pretty good, but it seems to be overfitting.\n",
    "\n",
    "### GoogLeNet Experiment 2: Data Augmentation\n",
    "\n",
    "Let's see if data augmentation helps with the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/70\n",
      "781/781 [==============================] - 36s 46ms/step - loss: 1.4885 - accuracy: 0.4570 - val_loss: 1.3241 - val_accuracy: 0.5350\n",
      "Epoch 2/70\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.0807 - accuracy: 0.6133 - val_loss: 1.0269 - val_accuracy: 0.6423\n",
      "Epoch 3/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.8980 - accuracy: 0.6833 - val_loss: 1.2337 - val_accuracy: 0.5979\n",
      "Epoch 4/70\n",
      "781/781 [==============================] - 287s 368ms/step - loss: 0.7859 - accuracy: 0.7247 - val_loss: 0.8290 - val_accuracy: 0.7109\n",
      "Epoch 5/70\n",
      "781/781 [==============================] - 38s 49ms/step - loss: 0.7043 - accuracy: 0.7562 - val_loss: 0.9168 - val_accuracy: 0.7040\n",
      "Epoch 6/70\n",
      "781/781 [==============================] - 32s 40ms/step - loss: 0.6400 - accuracy: 0.7788 - val_loss: 0.7356 - val_accuracy: 0.7498\n",
      "Epoch 7/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.5908 - accuracy: 0.7991 - val_loss: 0.5919 - val_accuracy: 0.7986\n",
      "Epoch 8/70\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.5529 - accuracy: 0.8102 - val_loss: 0.5287 - val_accuracy: 0.8157\n",
      "Epoch 9/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.5130 - accuracy: 0.8233 - val_loss: 0.6551 - val_accuracy: 0.7804\n",
      "Epoch 10/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.4841 - accuracy: 0.8326 - val_loss: 0.6248 - val_accuracy: 0.7923\n",
      "Epoch 11/70\n",
      "781/781 [==============================] - 34s 43ms/step - loss: 0.4549 - accuracy: 0.8442 - val_loss: 0.5544 - val_accuracy: 0.8125\n",
      "Epoch 12/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.4276 - accuracy: 0.8539 - val_loss: 0.5215 - val_accuracy: 0.8232\n",
      "Epoch 13/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.4073 - accuracy: 0.8597 - val_loss: 0.5667 - val_accuracy: 0.8098\n",
      "Epoch 14/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.3831 - accuracy: 0.8684 - val_loss: 0.5344 - val_accuracy: 0.8280\n",
      "Epoch 15/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.3667 - accuracy: 0.8748 - val_loss: 0.5955 - val_accuracy: 0.8113\n",
      "Epoch 16/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.3534 - accuracy: 0.8791 - val_loss: 0.6372 - val_accuracy: 0.8000\n",
      "Epoch 17/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.3360 - accuracy: 0.8840 - val_loss: 0.6217 - val_accuracy: 0.8066\n",
      "Epoch 18/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.3153 - accuracy: 0.8925 - val_loss: 0.5073 - val_accuracy: 0.8415\n",
      "Epoch 19/70\n",
      "781/781 [==============================] - 34s 44ms/step - loss: 0.3081 - accuracy: 0.8948 - val_loss: 0.4990 - val_accuracy: 0.8434\n",
      "Epoch 20/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.2899 - accuracy: 0.8992 - val_loss: 0.5315 - val_accuracy: 0.8339\n",
      "Epoch 21/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.2787 - accuracy: 0.9030 - val_loss: 0.4541 - val_accuracy: 0.8515\n",
      "Epoch 22/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.2674 - accuracy: 0.9073 - val_loss: 0.4498 - val_accuracy: 0.8575\n",
      "Epoch 23/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.2540 - accuracy: 0.9121 - val_loss: 0.5526 - val_accuracy: 0.8261\n",
      "Epoch 24/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.2431 - accuracy: 0.9150 - val_loss: 0.4986 - val_accuracy: 0.8536\n",
      "Epoch 25/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.2363 - accuracy: 0.9183 - val_loss: 0.4999 - val_accuracy: 0.8404\n",
      "Epoch 26/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.2239 - accuracy: 0.9226 - val_loss: 0.4601 - val_accuracy: 0.8543\n",
      "Epoch 27/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.2147 - accuracy: 0.9248 - val_loss: 0.5616 - val_accuracy: 0.8415\n",
      "Epoch 28/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.2069 - accuracy: 0.9281 - val_loss: 0.4426 - val_accuracy: 0.8612\n",
      "Epoch 29/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.1963 - accuracy: 0.9323 - val_loss: 0.5748 - val_accuracy: 0.8440\n",
      "Epoch 30/70\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.1866 - accuracy: 0.9357 - val_loss: 0.4577 - val_accuracy: 0.8625\n",
      "Epoch 31/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1815 - accuracy: 0.9374 - val_loss: 0.5142 - val_accuracy: 0.8529\n",
      "Epoch 32/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1769 - accuracy: 0.9389 - val_loss: 0.4344 - val_accuracy: 0.8674\n",
      "Epoch 33/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1655 - accuracy: 0.9417 - val_loss: 0.4369 - val_accuracy: 0.8662\n",
      "Epoch 34/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1560 - accuracy: 0.9451 - val_loss: 0.4728 - val_accuracy: 0.8643\n",
      "Epoch 35/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1526 - accuracy: 0.9470 - val_loss: 0.4316 - val_accuracy: 0.8724\n",
      "Epoch 36/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1472 - accuracy: 0.9494 - val_loss: 0.4312 - val_accuracy: 0.8726\n",
      "Epoch 37/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1364 - accuracy: 0.9522 - val_loss: 0.4678 - val_accuracy: 0.8695\n",
      "Epoch 38/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1315 - accuracy: 0.9554 - val_loss: 0.5301 - val_accuracy: 0.8546\n",
      "Epoch 39/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.1243 - accuracy: 0.9578 - val_loss: 0.4176 - val_accuracy: 0.8806\n",
      "Epoch 40/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1194 - accuracy: 0.9590 - val_loss: 0.4525 - val_accuracy: 0.8776\n",
      "Epoch 41/70\n",
      "781/781 [==============================] - 34s 44ms/step - loss: 0.1143 - accuracy: 0.9614 - val_loss: 0.5994 - val_accuracy: 0.8495\n",
      "Epoch 42/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1117 - accuracy: 0.9627 - val_loss: 0.4881 - val_accuracy: 0.8687\n",
      "Epoch 43/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.1097 - accuracy: 0.9625 - val_loss: 0.3934 - val_accuracy: 0.8863\n",
      "Epoch 44/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.1015 - accuracy: 0.9647 - val_loss: 0.4375 - val_accuracy: 0.8785\n",
      "Epoch 45/70\n",
      "781/781 [==============================] - 33s 43ms/step - loss: 0.0976 - accuracy: 0.9670 - val_loss: 0.4390 - val_accuracy: 0.8770\n",
      "Epoch 46/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0939 - accuracy: 0.9678 - val_loss: 0.4563 - val_accuracy: 0.8787\n",
      "Epoch 47/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0885 - accuracy: 0.9698 - val_loss: 0.4204 - val_accuracy: 0.8897\n",
      "Epoch 48/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0842 - accuracy: 0.9718 - val_loss: 0.4635 - val_accuracy: 0.8767\n",
      "Epoch 49/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.0792 - accuracy: 0.9736 - val_loss: 0.4152 - val_accuracy: 0.8829\n",
      "Epoch 50/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0759 - accuracy: 0.9750 - val_loss: 0.4024 - val_accuracy: 0.8933\n",
      "Epoch 51/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0725 - accuracy: 0.9769 - val_loss: 0.4077 - val_accuracy: 0.8898\n",
      "Epoch 52/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0672 - accuracy: 0.9776 - val_loss: 0.4944 - val_accuracy: 0.8753\n",
      "Epoch 53/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0631 - accuracy: 0.9795 - val_loss: 0.4167 - val_accuracy: 0.8892\n",
      "Epoch 54/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0607 - accuracy: 0.9798 - val_loss: 0.4305 - val_accuracy: 0.8898\n",
      "Epoch 55/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0590 - accuracy: 0.9807 - val_loss: 0.4301 - val_accuracy: 0.8892\n",
      "Epoch 56/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0553 - accuracy: 0.9826 - val_loss: 0.4199 - val_accuracy: 0.8899\n",
      "Epoch 57/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0556 - accuracy: 0.9818 - val_loss: 0.4213 - val_accuracy: 0.8912\n",
      "Epoch 58/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.0517 - accuracy: 0.9835 - val_loss: 0.4138 - val_accuracy: 0.8947\n",
      "Epoch 59/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0494 - accuracy: 0.9846 - val_loss: 0.4567 - val_accuracy: 0.8897\n",
      "Epoch 60/70\n",
      "781/781 [==============================] - 32s 40ms/step - loss: 0.0468 - accuracy: 0.9857 - val_loss: 0.4122 - val_accuracy: 0.8963\n",
      "Epoch 61/70\n",
      "781/781 [==============================] - 34s 43ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.4152 - val_accuracy: 0.8957\n",
      "Epoch 62/70\n",
      "781/781 [==============================] - 33s 43ms/step - loss: 0.0439 - accuracy: 0.9866 - val_loss: 0.4077 - val_accuracy: 0.8968\n",
      "Epoch 63/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0414 - accuracy: 0.9875 - val_loss: 0.4042 - val_accuracy: 0.8991\n",
      "Epoch 64/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0407 - accuracy: 0.9881 - val_loss: 0.4034 - val_accuracy: 0.8973\n",
      "Epoch 65/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0384 - accuracy: 0.9883 - val_loss: 0.3911 - val_accuracy: 0.8988\n",
      "Epoch 66/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0379 - accuracy: 0.9889 - val_loss: 0.4021 - val_accuracy: 0.8981\n",
      "Epoch 67/70\n",
      "781/781 [==============================] - 32s 42ms/step - loss: 0.0353 - accuracy: 0.9900 - val_loss: 0.3929 - val_accuracy: 0.8994\n",
      "Epoch 68/70\n",
      "781/781 [==============================] - 32s 41ms/step - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.3970 - val_accuracy: 0.8993\n",
      "Epoch 69/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0348 - accuracy: 0.9906 - val_loss: 0.3961 - val_accuracy: 0.9005\n",
      "Epoch 70/70\n",
      "781/781 [==============================] - 33s 42ms/step - loss: 0.0328 - accuracy: 0.9906 - val_loss: 0.3952 - val_accuracy: 0.8995\n",
      "Model: \"MiniGoogLeNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 96)   2688        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 96)   384         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 96)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   3104        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 32)   27680       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 64)   0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 48)   27696       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 15, 15, 80)   57680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 80)   320         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 80)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 15, 15, 80)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 15, 15, 160)  0           activation_5[0][0]               \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 15, 112)  18032       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 15, 15, 48)   69168       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 15, 112)  448         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 15, 48)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 112)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 15, 15, 160)  0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 96)   15456       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 15, 15, 64)   92224       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 15, 96)   384         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 96)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 15, 15, 160)  0           activation_8[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 15, 15, 80)   12880       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 80)   115280      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 80)   320         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 15, 80)   320         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 15, 80)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15, 15, 160)  0           activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 15, 15, 48)   7728        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 15, 15, 96)   138336      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 15, 48)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 15, 15, 96)   384         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 15, 48)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 15, 15, 96)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 15, 144)  0           activation_12[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 7, 7, 96)     124512      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 7, 7, 96)     384         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 7, 7, 96)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 144)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 7, 7, 240)    0           activation_14[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 176)    42416       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 7, 7, 160)    345760      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 7, 7, 176)    704         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 7, 7, 160)    640         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 7, 7, 176)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 7, 7, 160)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 7, 7, 336)    0           activation_15[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 7, 7, 176)    59312       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 160)    484000      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 7, 7, 176)    704         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 7, 7, 160)    640         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 7, 7, 176)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 7, 7, 160)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7, 7, 336)    0           activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 336)    0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1, 1, 336)    0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 336)          0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           3370        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 10)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,656,250\n",
      "Trainable params: 1,652,826\n",
      "Non-trainable params: 3,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      " Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8994    0.9300    0.9145      1000\n",
      "           1     0.9412    0.9610    0.9510      1000\n",
      "           2     0.8774    0.8660    0.8717      1000\n",
      "           3     0.8148    0.7920    0.8032      1000\n",
      "           4     0.8807    0.8860    0.8833      1000\n",
      "           5     0.8582    0.8410    0.8495      1000\n",
      "           6     0.9097    0.9470    0.9280      1000\n",
      "           7     0.9251    0.9010    0.9129      1000\n",
      "           8     0.9540    0.9340    0.9439      1000\n",
      "           9     0.9314    0.9370    0.9342      1000\n",
      "\n",
      "    accuracy                         0.8995     10000\n",
      "   macro avg     0.8992    0.8995    0.8992     10000\n",
      "weighted avg     0.8992    0.8995    0.8992     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19311623748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfEpochs = 70\n",
    "initialLearningRate = 0.005\n",
    "\n",
    "def polynomial_decay(epoch):\n",
    "    maxEpochs = numberOfEpochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "    power = 1.0\n",
    "    \n",
    "    alpha = baseLearningRate * (1 - (epoch / float(numberOfEpochs))) ** power\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "    \n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype('float')\n",
    "testX = testX.astype('float')\n",
    "\n",
    "# use mean subtraction\n",
    "mean = np.mean(trainX, axis = 0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "\n",
    "# convert labels to one-hot\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1,\n",
    "                         horizontal_flip = True, fill_mode=\"nearest\")\n",
    "\n",
    "callbacks = [LearningRateScheduler(polynomial_decay)]\n",
    "\n",
    "print('[INFO] compiling model...')\n",
    "opt = SGD(lr = initialLearningRate, momentum=0.9)\n",
    "model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=64), validation_data=(testX, testY),\n",
    "              steps_per_epoch=len(trainX) // 64, epochs=numberOfEpochs, callbacks = callbacks, verbose=1)\n",
    "\n",
    "# print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "# print a classification report\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY, digits=4))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now cracked 90% for the first time with CIFAR-10!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
